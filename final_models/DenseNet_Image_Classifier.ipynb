{"cells":[{"cell_type":"markdown","id":"62b2820b-37c9-41e2-a862-369f4df56f7c","metadata":{"id":"62b2820b-37c9-41e2-a862-369f4df56f7c"},"source":["# DenseNet121 Image Classifier"]},{"cell_type":"markdown","source":["**This notebook should run correctly both in colab and on jupyter.**\n","- Uses stratified sampling (making sure all classes have same ratio in all sets).\n","- Uses metadata from **'data/encoded_columns_data'**.\n","- Uses images from folder **'directly_processed_images'**."],"metadata":{"id":"-t0zJFfIJSkG"},"id":"-t0zJFfIJSkG"},{"cell_type":"markdown","source":["## **Setup**"],"metadata":{"id":"j37uiA6Oshq0"},"id":"j37uiA6Oshq0"},{"cell_type":"code","source":["import sys\n","on_colab = 'google.colab' in sys.modules\n","if on_colab:\n","  print(\"Running notebook on google colab\")\n","  # use ice cream to conveniently print stuff\n","  !pip install icecream\n","else:\n","  print(\"Notebook not running on google colab\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gDLEcekJqKJm","executionInfo":{"status":"ok","timestamp":1713194076092,"user_tz":-480,"elapsed":6636,"user":{"displayName":"Janice Yohana","userId":"10056001383705918670"}},"outputId":"096cc9b3-1621-4ee2-ab49-0cf505aadf97"},"id":"gDLEcekJqKJm","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running notebook on google colab\n","Requirement already satisfied: icecream in /usr/local/lib/python3.10/dist-packages (2.1.3)\n","Requirement already satisfied: colorama>=0.3.9 in /usr/local/lib/python3.10/dist-packages (from icecream) (0.4.6)\n","Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from icecream) (2.16.1)\n","Requirement already satisfied: executing>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from icecream) (2.0.1)\n","Requirement already satisfied: asttokens>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from icecream) (2.4.1)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from asttokens>=2.0.1->icecream) (1.16.0)\n"]}]},{"cell_type":"code","execution_count":null,"id":"789a21a2-6b2f-43f3-9fe1-6e3cacb6f1b6","metadata":{"id":"789a21a2-6b2f-43f3-9fe1-6e3cacb6f1b6"},"outputs":[],"source":["# imports\n","from icecream import ic\n","import pandas as pd\n","import numpy as np\n","import torch\n","import time\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import models, datasets, transforms\n","import timeit\n","import torchvision\n","# import torchsummary\n","from torchvision import transforms, datasets\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","# from torchsummary import summary\n","import os\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.model_selection import train_test_split\n","from PIL import Image\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# to save session\n","import pickle\n","\n","from tqdm import tqdm"]},{"cell_type":"code","source":["if on_colab:\n","  from google.colab import drive\n","  drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tKHCqZRTp9EX","executionInfo":{"status":"ok","timestamp":1713194078979,"user_tz":-480,"elapsed":2896,"user":{"displayName":"Janice Yohana","userId":"10056001383705918670"}},"outputId":"2bf1e94e-1438-4ef4-ddd9-687a5e319dd8"},"id":"tKHCqZRTp9EX","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"id":"5faac262-b250-4155-93b8-b06e1b0ff8f5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5faac262-b250-4155-93b8-b06e1b0ff8f5","executionInfo":{"status":"ok","timestamp":1713194078980,"user_tz":-480,"elapsed":9,"user":{"displayName":"Janice Yohana","userId":"10056001383705918670"}},"outputId":"6a48217f-fe92-4994-fefd-1c35d912926c"},"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch version:\n","2.2.1+cu121\n","Using cuda device\n"]}],"source":["print(\"PyTorch version:\")\n","print(torch.__version__)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using {} device\".format(device))"]},{"cell_type":"markdown","source":["## Define paths to image folder & metadata"],"metadata":{"id":"JnPFr5uzs1b6"},"id":"JnPFr5uzs1b6"},{"cell_type":"code","source":["# paths\n","if on_colab:\n","  metadata_path = '/content/drive/MyDrive/CDS Project/data/encoded_final_data.csv'\n","  images_path = '/content/drive/MyDrive/CDS Project/directly_processed_images'\n","else:\n","  metadata_path = '../data/encoded_final_data.csv'\n","  images_path = '../directly_processed_images'"],"metadata":{"id":"BLVcBUgI3uLo"},"id":"BLVcBUgI3uLo","execution_count":null,"outputs":[]},{"cell_type":"code","source":["metadata_df = pd.read_csv(metadata_path)\n","metadata_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":304},"id":"s0e9yKmfIiY_","executionInfo":{"status":"error","timestamp":1713194078980,"user_tz":-480,"elapsed":8,"user":{"displayName":"Janice Yohana","userId":"10056001383705918670"}},"outputId":"becc8cc7-3f9f-4e92-af12-18af0db956e8"},"id":"s0e9yKmfIiY_","execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/CDS Project/data/encoded_final_data.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-0fb60729f50d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetadata_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmetadata_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/CDS Project/data/encoded_final_data.csv'"]}]},{"cell_type":"code","source":["num_images = len([file for file in os.listdir(images_path) if os.path.isfile(os.path.join(images_path, file))])\n","print(\"Number of images: \", num_images)"],"metadata":{"id":"4JFB86Agr4uv"},"id":"4JFB86Agr4uv","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train test split (Stratified Sampling)"],"metadata":{"id":"d_wH_gDXTGX2"},"id":"d_wH_gDXTGX2"},{"cell_type":"code","execution_count":null,"id":"a1fe1e4f-a5cd-42a8-ab39-dee09e1a8dcb","metadata":{"id":"a1fe1e4f-a5cd-42a8-ab39-dee09e1a8dcb"},"outputs":[],"source":["# Split data in features and target:\n","X = metadata_df  # features\n","y = metadata_df['target']  # target\n","\n","# Splitting into train, test, and validation sets with stratified sampling\n","X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=0, stratify=y_temp)\n","\n","ic(len(X_train), len(X_test), len(X_val))"]},{"cell_type":"code","execution_count":null,"id":"18a3cbe6-3569-4885-9b28-d70b31c8962b","metadata":{"id":"18a3cbe6-3569-4885-9b28-d70b31c8962b"},"outputs":[],"source":["# Resizing to 224x224, some transformations/augmentation\n","train_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.RandomRotation(15),\n","    transforms.RandomCrop(224, padding=4),\n","    # transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2), # random adjustments\n","    # transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),  # for blurry images\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # standard normalization\n","])\n","\n","val_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])"]},{"cell_type":"markdown","source":["## Define CustomDataset function"],"metadata":{"id":"Cu0Egx0fSPIB"},"id":"Cu0Egx0fSPIB"},{"cell_type":"code","execution_count":null,"id":"0e9ec097-8eef-43e6-a814-b78e377d80a0","metadata":{"id":"0e9ec097-8eef-43e6-a814-b78e377d80a0"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, metadata, transform=None):\n","        self.metadata = metadata\n","        self.images_path = images_path\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.metadata)\n","\n","    def __getitem__(self, idx):\n","        img_id = self.metadata.iloc[idx, 0]  # Get image ID of sample using the metadata index\n","        img_name = os.path.join(self.images_path, f'processed_{img_id}.jpg')  # name of image files\n","        if not os.path.exists(img_name):\n","            raise FileNotFoundError(f\"Image file not found: {img_name}\")\n","\n","        image = Image.open(img_name)\n","        row = metadata_df[metadata_df['image_id'] == img_id]\n","        label = row.iloc[0]['target']\n","\n","        if self.transform is not None:\n","            image = self.transform(image)\n","\n","        # # extract demographic features\n","        # age = torch.tensor(self.metadata.iloc[idx, 1])\n","        # localization = torch.tensor(self.metadata.iloc[idx, 3])\n","        # gender = torch.tensor(self.metadata.iloc[idx, 4])\n","\n","        return image, label"]},{"cell_type":"markdown","source":["## Define train, validation and test datasets/dataloaders"],"metadata":{"id":"Gx1fyj5oSYkX"},"id":"Gx1fyj5oSYkX"},{"cell_type":"code","source":["# custom datasets\n","train_dataset = CustomDataset(metadata=X_train, transform=train_transform)\n","val_dataset = CustomDataset(metadata=X_val, transform=val_transform)\n","test_dataset = CustomDataset(metadata=X_test, transform=val_transform)\n","\n","# Create data loaders (optimized for faster loading)\n","batch_size = 32\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, persistent_workers=True, pin_memory=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8, persistent_workers=True, pin_memory=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8, persistent_workers=True, pin_memory=True)\n","\n","# check\n","ic(len(train_dataset), len(val_dataset), len(test_dataset))"],"metadata":{"id":"q_kZ-NCW-68Q"},"id":"q_kZ-NCW-68Q","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test to check some samples in split sets\n","for index in range(20):\n","\n","  image, label = train_dataset[index]\n","\n","  # Display information about the sample\n","  # print(\"Random Sample Index:\", index)\n","  # print(\"Image Shape:\", image.shape)\n","  # print(\"Label:\", label)\n","  # print(type(label))\n","\n","  # Display the image\n","  plt.imshow(image.permute(1, 2, 0))  # permute to rearrange dimensions from CxHxW to HxWxC\n","  plt.title(f\"Label: {label}\")\n","  plt.axis('off')\n","  plt.show()"],"metadata":{"id":"hdhbiC6J_VUy"},"id":"hdhbiC6J_VUy","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"d1676d70-7c5f-4161-b0e8-b6231611a15f","metadata":{"id":"d1676d70-7c5f-4161-b0e8-b6231611a15f"},"source":["## DenseNet121 + modified last layer"]},{"cell_type":"code","execution_count":null,"id":"dba72072-5a22-4817-a879-3ccf5774a2e2","metadata":{"id":"dba72072-5a22-4817-a879-3ccf5774a2e2"},"outputs":[],"source":["model_densenet = models.densenet121(pretrained=True)"]},{"cell_type":"markdown","id":"662163da-0a45-48a9-b035-cfa6c7804c5a","metadata":{"id":"662163da-0a45-48a9-b035-cfa6c7804c5a"},"source":["#### Modify last layer\n","- adjust to the appropriate number of output classes (7)"]},{"cell_type":"code","execution_count":null,"id":"b4dd57ef-0dfc-4866-ad8d-50bef8db1d51","metadata":{"id":"b4dd57ef-0dfc-4866-ad8d-50bef8db1d51"},"outputs":[],"source":["num_features = model_densenet.classifier.in_features\n","model_densenet.classifier = nn.Linear(num_features, 7)\n","\n","model_densenet # new model structure"]},{"cell_type":"markdown","id":"e95e0f6a-7911-486d-99b7-bad68a235e1c","metadata":{"id":"e95e0f6a-7911-486d-99b7-bad68a235e1c"},"source":["#### Adam optimizer + CEL"]},{"cell_type":"code","execution_count":null,"id":"73213526-edea-4934-b1e1-ded1abb5d5cc","metadata":{"id":"73213526-edea-4934-b1e1-ded1abb5d5cc"},"outputs":[],"source":["# Loss function: CEL\n","criterion = nn.CrossEntropyLoss()\n","\n","# Optimizer: Adam\n","optimizer = torch.optim.Adam(model_densenet.parameters(), lr=0.001, )"]},{"cell_type":"markdown","source":["### Training"],"metadata":{"id":"tivqFwF0SfaC"},"id":"tivqFwF0SfaC"},{"cell_type":"code","source":["model_densenet.to(device)\n","criterion.to(device)"],"metadata":{"id":"RxUbeI7yhVVV"},"id":"RxUbeI7yhVVV","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training for one epoch\n","\n","def train_model(model, criterion, optimizer, train_loader, val_loader):\n","\n","    model.train()\n","    train_loss = 0.0\n","    correct_train = 0\n","    total_train = 0\n","\n","    for images, labels in tqdm(train_loader):\n","        labels = labels.long()\n","        labels = labels.to(device)\n","        images = images.to(device)\n","        optimizer.zero_grad()\n","        # forward propagation, i.e. get predictions\n","        outputs = model(images)\n","        # calculate loss and backpropagate to model paramters:\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item() * images.size(0)\n","        _, predicted = torch.max(outputs, 1)\n","        total_train += labels.size(0)\n","        correct_train += (predicted == labels).sum().item()\n","\n","    train_loss /= len(train_loader.dataset)\n","    train_acc = correct_train / total_train\n","\n","    model.eval()\n","    val_loss = 0.0\n","    correct_val = 0\n","    total_val = 0\n","\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            labels=labels.long()\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item() * images.size(0)\n","            _, predicted = torch.max(outputs, 1)\n","            total_val += labels.size(0)\n","            correct_val += (predicted == labels).sum().item()\n","\n","    val_loss /= len(val_loader.dataset)\n","    val_acc = correct_val / total_val\n","\n","    return train_loss, val_loss, train_acc, val_acc"],"metadata":{"id":"EGIXSXUo-G7Z"},"id":"EGIXSXUo-G7Z","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# specify epochs to train\n","total_epochs = 40\n","\n","tl = []\n","vl = []\n","ta = []\n","va = []\n","\n","for epoch in range(total_epochs):\n","    start_time = time.time()  # Record start time\n","\n","    train_loss, val_loss, train_acc, val_acc = train_model(model_densenet, criterion, optimizer, train_loader, val_loader)\n","    tl.append(train_loss)\n","    vl.append(val_loss)\n","    ta.append(train_acc)\n","    va.append(val_acc)\n","\n","    elapsed_time = time.time() - start_time  # Calculate elapsed time\n","\n","    print(f\"Epoch {epoch+1}/{total_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Elapsed Time: {elapsed_time:.2f} seconds\")"],"metadata":{"id":"lISck43p-G4j"},"id":"lISck43p-G4j","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# extra epochs (if needed)\n","\n","for epoch in range(10):\n","    start_time = time.time()  # Record start time\n","\n","    train_loss, val_loss, train_acc, val_acc = train_model(model_densenet, criterion, optimizer, train_loader, val_loader)\n","    tl.append(train_loss)\n","    vl.append(val_loss)\n","    ta.append(train_acc)\n","    va.append(val_acc)\n","\n","    elapsed_time = time.time() - start_time  # Calculate elapsed time\n","\n","    print(f\"Epoch {40+epoch+1}/{50}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Elapsed Time: {elapsed_time:.2f} seconds\")"],"metadata":{"id":"T7OhGgYL3Qq1"},"id":"T7OhGgYL3Qq1","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Visualisation"],"metadata":{"id":"MMuF7Hl8TWd0"},"id":"MMuF7Hl8TWd0"},{"cell_type":"code","source":["# visualisation function\n","def plotResults():\n","    plt.figure(figsize=(10,6))\n","    epovec=range(len(ta))\n","    plt.plot(epovec,ta,epovec,tl,epovec,va,epovec,vl,linewidth=3)\n","    plt.legend(('Training Accuracy','Training Loss','Validation Accuracy','Validation Loss'))\n","\n","    # make the graph understandable:\n","    plt.title(\"Losses and Accuracies\")\n","    plt.xlabel(\"Epoch #\")\n","    plt.ylabel(\"Loss/Accuracy\")\n","    plt.show()"],"metadata":{"id":"CaKfe7a3fski"},"id":"CaKfe7a3fski","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot loss function\n","def plotLosses():\n","    plt.figure(figsize=(10,6))\n","    epovec=range(len(tl))\n","    plt.plot(epovec,tl,epovec,vl,linewidth=3)\n","    plt.legend(('Training Loss', 'Validation Loss'))\n","\n","    # make the graph understandable:\n","    plt.title(\"Training and Validation Losses\")\n","    plt.xlabel(\"Epoch #\")\n","    plt.ylabel(\"Loss\")\n","    plt.show()"],"metadata":{"id":"a7xyl98V3pFu"},"id":"a7xyl98V3pFu","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot accuracies function\n","def plotAccuracies():\n","    plt.figure(figsize=(10,6))\n","    epovec=range(len(ta))\n","    plt.plot(epovec,ta,epovec,va,linewidth=3)\n","    plt.legend(('Training Accuracy', 'Validation Accuracy'))\n","\n","    # make the graph understandable:\n","    plt.title(\"Training and Validation Accuracies\")\n","    plt.xlabel(\"Epoch #\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.show()"],"metadata":{"id":"VoQ5tCb939yC"},"id":"VoQ5tCb939yC","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# evaluate model function\n","def evaluate_model(model, criterion, data_loader):\n","    model.eval()\n","    loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    all_predictions = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for images, labels in data_loader:\n","            labels = labels.long()\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            loss += criterion(outputs, labels).item() * images.size(0)\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","            all_predictions.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    loss /= len(data_loader.dataset)\n","    accuracy = correct / total\n","\n","    return loss, accuracy, all_predictions, all_labels"],"metadata":{"id":"52hvz43-50fE"},"id":"52hvz43-50fE","execution_count":null,"outputs":[]},{"cell_type":"code","source":["plotResults()"],"metadata":{"id":"kt4ZdZbjdxmX"},"id":"kt4ZdZbjdxmX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot losses separately\n","plotLosses()"],"metadata":{"id":"hDlQhi2l3aoX"},"id":"hDlQhi2l3aoX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot accuracies separately\n","plotAccuracies()"],"metadata":{"id":"eYIGsAvU3afX"},"id":"eYIGsAvU3afX","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation on Test Set"],"metadata":{"id":"WlMpFkdl0GkE"},"id":"WlMpFkdl0GkE"},{"cell_type":"code","source":["from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Evaluate model\n","test_loss, test_accuracy, all_predictions, all_labels = evaluate_model(model_densenet, criterion, test_loader)\n","\n","# Compute classification report\n","report = classification_report(all_labels, all_predictions)\n","print(\"Classification Report:\\n\", report)\n","\n","# Compute confusion matrix\n","conf_matrix = confusion_matrix(all_labels, all_predictions)\n","print(\"Confusion Matrix:\\n\", conf_matrix)\n","\n","# Compute accuracy per class\n","class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n","print(\"Accuracy per Class:\", class_accuracy)\n","print(\"Test accuracy: \", test_accuracy)"],"metadata":{"id":"MyazaHvheJ9d"},"id":"MyazaHvheJ9d","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Save model & parameters"],"metadata":{"id":"Xb9lliZwhuoj"},"id":"Xb9lliZwhuoj"},{"cell_type":"code","source":["# define saving paths\n","if on_colab:\n","  save_to_path = '/content/drive/MyDrive/CDS Project/saved_models/densenet_classifier_40epochs.pth'\n","  save_params_path = '/content/drive/MyDrive/CDS Project/saved_models/densenet_classifier_40epochs_params.pth'\n","else:\n","  save_to_path = '../saved_models/densenet_classifier_40epochs.pth'\n","  save_params_path = '../saved_models/densenet_classifier_40epochs_params.pth'"],"metadata":{"id":"e_5lgNXj4bLm"},"id":"e_5lgNXj4bLm","execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model_densenet, save_to_path)\n","torch.save(model_densenet.state_dict(), save_params_path)"],"metadata":{"id":"L4JWtmNkUXvN"},"id":"L4JWtmNkUXvN","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZmHU3Pba-EKh"},"id":"ZmHU3Pba-EKh","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}